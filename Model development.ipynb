{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f334e6a5-44ab-4c94-9367-6c3f7e67d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL DEVELOPMENT PIPELINE FOR TNBC SUBTYPE CLASSIFICATION\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. LOAD SAVED CLUSTERING OUTPUT\n",
    "# ------------------------------------------------------------\n",
    "with open(\"clustering_output.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train_df = data[\"X_train_df\"]\n",
    "X_test_df  = data[\"X_test_df\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. LOAD DEG FEATURE LISTS\n",
    "# ------------------------------------------------------------\n",
    "files = [\n",
    "    \"significant_genes_C1_v2.csv\",\n",
    "    \"significant_genes_C2_v2.csv\",\n",
    "    \"significant_genes_C3_v2.csv\",\n",
    "    \"significant_genes_C4_v2.csv\"\n",
    "]\n",
    "\n",
    "dfs = [pd.read_csv(f, index_col=0) for f in files]\n",
    "for df in dfs:\n",
    "    df.index.name = None\n",
    "\n",
    "# DEG union\n",
    "df = pd.concat(dfs, axis=1, join=\"outer\")\n",
    "\n",
    "# Load subtypes assigned from clustering\n",
    "subs = pd.read_csv(\"only_subtypes_v2.csv\")\n",
    "subs.set_index(\"Unnamed: 0\", inplace=True)\n",
    "subs.index.name = None\n",
    "\n",
    "df[\"subtype\"] = subs[\"subtype\"]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. DATA PREPROCESSING\n",
    "# ------------------------------------------------------------\n",
    "print(\"Missing values:\", df.isna().all().sum())\n",
    "print(df[\"subtype\"].value_counts())\n",
    "\n",
    "X_train_deg = df.drop(columns=[\"subtype\"])\n",
    "y_train      = df[\"subtype\"]\n",
    "\n",
    "y_test = X_test_df[\"subtype\"]\n",
    "X_test_df = X_test_df.drop(columns=[\"subtype\"])\n",
    "\n",
    "# Remove duplicate gene names\n",
    "X_train_deg = X_train_deg.groupby(X_train_deg.columns, axis=1).mean()\n",
    "X_test_df   = X_test_df.groupby(X_test_df.columns, axis=1).mean()\n",
    "\n",
    "# Remove highly correlated genes\n",
    "corr = X_train_deg.corr()\n",
    "upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "to_drop = [col for col in upper.columns if any(upper[col] > 0.80)]\n",
    "X_train_filtered = X_train_deg.drop(columns=to_drop)\n",
    "\n",
    "deg_genes = X_train_deg.columns.tolist()\n",
    "X_test_deg = X_test_df[deg_genes]\n",
    "X_test_filtered = X_test_deg.drop(columns=to_drop)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train)\n",
    "y_test_encoded  = le.transform(y_test)\n",
    "\n",
    "print(\"Label Mapping:\")\n",
    "for o, e in zip(le.classes_, le.transform(le.classes_)):\n",
    "    print(f\"{o} → {e}\")\n",
    "\n",
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "col_train = X_train_filtered.columns\n",
    "col_test  = X_test_filtered.columns\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train_filtered)\n",
    "X_test_scaled  = scaler.transform(X_test_filtered)\n",
    "\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=col_train)\n",
    "X_test_scaled_df  = pd.DataFrame(X_test_scaled,  columns=col_test)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. RANDOM FOREST RFE FEATURE SELECTION\n",
    "# ------------------------------------------------------------\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_features_to_select = 50  # Adjust as needed\n",
    "\n",
    "start_time = time.time()\n",
    "rfe = RFE(estimator=rf, n_features_to_select=n_features_to_select, verbose=1)\n",
    "rfe.fit(X_train_scaled_df, y_train)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"RFE completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "selected_features = rfe.support_\n",
    "X_train_rfe = X_train_scaled_df.loc[:, selected_features]\n",
    "X_test_rfe  = X_test_scaled_df.loc[:, selected_features]\n",
    "\n",
    "top_features = X_train_filtered.columns[selected_features]\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. TRAIN MODELS (RF, XGB, DT, SVM) — CROSS-VALIDATION\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "models = {\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=100, criterion=\"entropy\",\n",
    "                                           random_state=42, n_jobs=-1),\n",
    "    \"XGB\": XGBClassifier(objective=\"multi:softmax\", num_class=5, random_state=42),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"SVM\": SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "X_train_np = X_train_rfe.values\n",
    "y_train_np = np.array(y_train_encoded)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for model_name, clf in models.items():\n",
    "    print(\"\\n===============================\")\n",
    "    print(f\"Training model: {model_name}\")\n",
    "    print(\"===============================\")\n",
    "\n",
    "    all_accuracy, all_sensitivity, all_specificity, all_precision = [], [], [], []\n",
    "    best_model = None\n",
    "    best_acc = 0\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_np, y_train_np), 1):\n",
    "        X_tr, X_val = X_train_np[train_idx], X_train_np[val_idx]\n",
    "        y_tr, y_val = y_train_np[train_idx], y_train_np[val_idx]\n",
    "\n",
    "        clf.fit(X_tr, y_tr)\n",
    "        y_pred = clf.predict(X_val)\n",
    "\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        sensitivity = []\n",
    "        specificity = []\n",
    "        precision   = []\n",
    "        accuracy    = []\n",
    "\n",
    "        for i in range(cm.shape[0]):\n",
    "            TP = cm[i, i]\n",
    "            FP = cm[:, i].sum() - TP\n",
    "            FN = cm[i, :].sum() - TP\n",
    "            TN = cm.sum() - TP - FP - FN\n",
    "\n",
    "            sensitivity.append(TP/(TP+FN) if TP+FN>0 else 0)\n",
    "            specificity.append(TN/(TN+FP) if TN+FP>0 else 0)\n",
    "            precision.append(TP/(TP+FP) if TP+FP>0 else 0)\n",
    "            accuracy.append((TP+TN)/cm.sum())\n",
    "\n",
    "        fold_acc = np.mean(accuracy)\n",
    "        all_accuracy.append(fold_acc)\n",
    "        all_sensitivity.append(np.mean(sensitivity))\n",
    "        all_specificity.append(np.mean(specificity))\n",
    "        all_precision.append(np.mean(precision))\n",
    "\n",
    "        if fold_acc > best_acc:\n",
    "            best_acc = fold_acc\n",
    "            best_model = clf\n",
    "\n",
    "        print(f\"Fold {fold}: Accuracy = {fold_acc:.4f}\")\n",
    "\n",
    "    results[model_name] = {\n",
    "        \"accuracy\": np.mean(all_accuracy),\n",
    "        \"sensitivity\": np.mean(all_sensitivity),\n",
    "        \"specificity\": np.mean(all_specificity),\n",
    "        \"precision\": np.mean(all_precision),\n",
    "        \"best_model\": best_model\n",
    "    }\n",
    "\n",
    "    print(\"\\n--- CV SUMMARY ---\")\n",
    "    print(f\"Accuracy:   {np.mean(all_accuracy):.4f}\")\n",
    "    print(f\"Sensitivity:{np.mean(all_sensitivity):.4f}\")\n",
    "    print(f\"Specificity:{np.mean(all_specificity):.4f}\")\n",
    "    print(f\"Precision:  {np.mean(all_precision):.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. TEST SET EVALUATION (RF SHAP ONLY)\n",
    "# ------------------------------------------------------------\n",
    "rf_best = results[\"RandomForest\"][\"best_model\"]\n",
    "y_test_pred = rf_best.predict(X_test_rfe)\n",
    "\n",
    "print(\"\\nConfusion Matrix (TEST SET, RF):\")\n",
    "print(confusion_matrix(y_test_encoded, y_test_pred))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. SHAP ANALYSIS FOR RANDOM FOREST\n",
    "# ------------------------------------------------------------\n",
    "explainer = shap.TreeExplainer(rf_best)\n",
    "feature_names = top_features.tolist()\n",
    "\n",
    "X_train_df_shap = pd.DataFrame(X_train_rfe, columns=feature_names)\n",
    "\n",
    "shap_values = explainer.shap_values(X_train_df_shap)\n",
    "shap_values_corrected = np.moveaxis(shap_values, -1, 0)\n",
    "\n",
    "for i in range(len(shap_values_corrected)):\n",
    "    plt.title(f\"SHAP Summary Plot – Class {i}\")\n",
    "    shap.summary_plot(shap_values_corrected[i], X_train_df_shap, show=False)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
